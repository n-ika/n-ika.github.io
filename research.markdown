---
layout: page
title: Research
permalink: /research/
---

<br>

---
<br>

*My Master Thesis was modelling non native speech perception and comparing it to human performance. The main idea behind was to explore where the mis-perception of non native speech sounds happen.*

Jurov, N. (2019). Phonetics or Phonology? Modelling Non-Native Perception (Unpublished Master thesis). University of Paris, Paris, France. [https://github.com/n-ika/abx_test_strut/blob/master/jurov_memoire.pdf](https://github.com/n-ika/abx_test_strut/blob/master/jurov_memoire.pdf)

<span style="font-size:0.8em;">Perceptual differences of non native speech have been discussed extensively in the literature before. In order to shed a light on where the mis-perception occurs (phonetic/low-level or phonology/higher cognitive level), I used several models and compared them to human performance. I used an ABX discrimination task of non words (either English or French possible CVCs) on French and English natives and I was looking for a native language influence in non native speech perception. I found out that unsupervised model (k-means clustering) shows some native language effect, while the supervised model (ASR - kaldi) does not. Both are outperformed by the acoustic baseline (MFCC vectors distance predictions). Overall, the best fit for human performance seem to be the universal phonetic transcriber - bottleneck features distances predictions, which have the highest accuracy in the ABX task and improve the k-means clustering results.</span>
<br>

---

<br>

*A part of my thesis is incorporated in the following article, presented at the CogSci 2019:*

Millet, J., Jurov, N., & Dunbar, E. (2019, June 16). Comparing unsupervised speech learning directly to human performance in speech perception. [https://doi.org/10.31234/osf.io/ake47](https://doi.org/10.31234/osf.io/ake47)

<span style="font-size:0.8em;">We compare the performance of humans (English and French listeners) versus an unsupervised speech model in a perception experiment (ABX discrimination task). Although the ABX task has been used for acoustic model evaluation in previous research, the results have not, until now, been compared directly with human behaviour in an experiment. We show that a standard, well-performing model (DPGMM) has better accuracy at predicting human responses than the acoustic baseline. The model also shows a native language effect, better resembling native listeners of the language on which it was trained. However, the native language effect shown by the models is different than the one shown by the human listeners, and, notably, the models do not show the same overall patterns of vowel confusions.</span>

<br>

---
